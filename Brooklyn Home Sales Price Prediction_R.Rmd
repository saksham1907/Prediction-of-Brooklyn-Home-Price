---
title: "Brooklyn Homes Sale Price"
author: "Saksham Agrawal"
date: "December 2, 2018"
output: html_document
---
```{r}
#Change the path before running the code chunk
path = "C:/Users/saksh/Downloads/ISEN 613 Notes/Project"
setwd(path)

install.packages("data.table")
install.packages("bit64")
library(data.table)
library(bit64)

data = fread("brooklyn_sales_map.csv")

dim(data)


```



```{r}
#Writing the data in dataframe format and finding the number and percentage of missing values for each attribute.
data = as.data.frame(data)

NAs = numeric()
percent_NAs = numeric()

for ( i in 1:ncol(data)){
    NAs[i] =sum(is.na(data[,i]))
    percent_NAs[i] = (NAs[i]/390883)*100
}

Missing_values= data.frame(Variable_Name = names(data), Total_Missing_Values = NAs, Percent_Values_missing =percent_NAs)

Missing_values

```
DATA CLEANING

The data contains a large number of variables which increase the noise and reduce the prediction capability of the model due to increased variance. We need to reduce the number of variables to the maximum before model fitting.

Going through the attribute description(attached as annexure), we can remove many variables on the following grounds.
1. Duplicacy -Reading from the description we find that many variables have same information but just different names. This is the case as the dataset is a product of concatenation of two different tables.

2. No Description- Many variables have no proper description. This may be the case as many variables are representations of various IDs of PLUTOMAP files which supposedly have no direct impact on the predictions made. 

3. Large percentage of mising data- We have discarded all variables where missinig values are more than 75% of all observations.

```{r}
data_stage2 = subset(data, select = -c(borough, Borough, UnitsRes, UnitsTotal, LotArea, BldgArea, BldgClass, Easements, easement, OwnerType, building_class_category, ZipCode, YearBuilt, MAPPLUTO_F, PLUTOMapID, SHAPE_Leng, SHAPE_Area, Address, EDesigNum,Version, Sanborn, ZoneMap, ZMCode, HistDist,Landmark, APPDate, FIRM07_FLA, PFIRM15_FL, Ext, AreaSource, sale_date,ZoneDist2, ZoneDist3, ZoneDist4, Overlay1, Overlay2, SPDist1, SPDist2, SPDist3, LtdHeight ))

dim(data_stage2)
```

We still have 70 attributes left in our data, which are still way too many for proper prediction.

Now, we check for 0s and NAs in our remaining data and impute them with median for numerical and mode for categorical data.

We find that Sale Price is 0 for transfer of property which does helps us in prediction so we can remove such observations

```{r}
transferred_properties = which(data_stage2$sale_price == 0)

data_stage2 = data_stage2[-(transferred_properties),]

dim(data_stage2)
```
```{r}
data_stage2 = subset(data_stage2, select = -c(OwnerName))
```

Let's look at the structure of our attributes

```{r}
str(data_stage2)
```

Conversting 'character' variables to 'factors'
```{r}

data_stage2$neighborhood = as.factor(data_stage2$neighborhood)  
data_stage2$tax_class = as.factor(data_stage2$tax_class)
data_stage2$building_class = as.factor(data_stage2$building_class)
data_stage2$address = as.factor(data_stage2$apartment_number)
data_stage2$building_class_at_sale =  as.factor(data_stage2$building_class_at_sale)
data_stage2$FireComp = as.factor(data_stage2$FireComp)
data_stage2$SanitSub = as.factor(data_stage2$SanitSub)
data_stage2$ZoneDist1 = as.factor(data_stage2$ZoneDist1)
data_stage2$SplitZone = as.factor(data_stage2$SplitZone)
data_stage2$IrrLotCode = as.factor(data_stage2$IrrLotCode)
data_stage2$apartment_number = as.factor(data_stage2$apartment_number)

str(data_stage2)
```

BBL and APPBBL also appear to be meant for identification purpose only. It is concatenation of the borough code, tax block and tax lot, which are already available to us separately.

```{r}
data_stage2 = subset(data_stage2, select = -c(BBL, APPBBL))
```

We can see there are same 4447 levels in 'address' and 'apartment_number'. Checking them more closely
```{r}
summary(data_stage2$address)
summary(data_stage2$apartment_number)
```

We find that both address and apartment_number entries are exactly the same. There are no entries for 193218 data observations. Moreover, they only represent the apartment_number and do not give any perception of the area or street or block where the property is present. So we can omit these attributes also.

```{r}
data_stage2 = subset(data_stage2, select = -c(address, apartment_number))

```
Let's check the number of missing values again for data cleaned uptil now.

```{r}
data = as.data.frame(data)

NAs_2 = numeric()
Type = character()

for ( i in 1:ncol(data_stage2)){
    NAs_2[i] =sum(is.na(data_stage2[,i]))
    Type[i] = class(data_stage2[,i])
}

Missing_values= data.frame(Variable_Name = names(data_stage2), Total_Missing_Values = NAs_2, Type= Type)

Missing_values

```

We keep a copy of data removing all observations with any variable NA.
```{r}
no_NA_data = na.omit(data_stage2)
```


Here onwards, solved using no NA values.

```{r}
no_NA_data = na.omit(data_stage2)
dim(no_NA_data)
```


We want to predict sales price of brooklyn holmes. Let's look at the scatter plot of the response variable.

```{r}
hist(no_NA_data$sale_price)
```
We see some outliers at both the ends.So we take the observations from 10 percentile to 90 percentile of sale price.

```{r}
quantile(no_NA_data$sale_price,c(0.1,0.9))
```


```{r}
no_NA_data_trimmed = subset(no_NA_data, sale_price>142524 & sale_price<1250500)

dim(no_NA_data_trimmed)

```
```{r}
hist(no_NA_data_trimmed$sale_price)
```

Now 65 attributes are fairly high to make a prediction with good accuracy. So we apply principal component analysis to break the reduce the number of varibles and capture most of the information in vital few variables.


```{r}
str(no_NA_data_trimmed)
```

UNIVARIATE ANALYSIS

Neighborhood- 
Department of Finance assessors determine the neighborhood name in the course of valuing properties. The common name of the neighborhood is generally the same as the name Finance designates. However, there may be slight differences in neighborhood boundary lines and some sub-neighborhoods may not be included.

```{r}
boxplot(sale_price~ neighborhood, no_NA_data_trimmed, col = "blue")
```

```{r}
dt = data.table(no_NA_data_trimmed)
nb_class =as.data.frame( dt[,list(mean= mean(sale_price)), by = neighborhood])
nb_class
nb_class$category = ifelse(nb_class$mean>300000 & nb_class$mean<452407.5, "low price neighborhood",ifelse
  (nb_class$mean>452407.5 & nb_class$mean<554618.1, "medium price neighborhood",ifelse
    (nb_class$mean>554618.1 & nb_class$mean<631312.3, "High price neighborhood", "Expensive Neighborhood")))

```
```{r}
#creating new categories for neighborhood
nb_class$category = ifelse(nb_class$mean>300000 & nb_class$mean<452407.5, "low price neighborhood",ifelse
  (nb_class$mean>452407.5 & nb_class$mean<554618.1, "medium price neighborhood",ifelse
    (nb_class$mean>554618.1 & nb_class$mean<631312.3, "High price neighborhood", "Expensive Neighborhood")))

```


```{r}
no_NA_data_trimmed = merge(no_NA_data_trimmed, nb_class[,c("neighborhood","category")])
boxplot(sale_price~category , no_NA_data_trimmed, col= "red")
```
Low price neighborhood has some outliers.

TAX_CLASS - 
Every property in the city is assigned to one of four tax classes (Classes 1, 2, 3, and 4), based on the use of the property.
. Class 1: Includes most residential property of up to three units (such as one-, two-, and three-family homes and small stores or offices with one or two attached apartments), vacant land that is zoned for residential use, and most condominiums that are not more than three stories.
. Class 2: Includes all other property that is primarily residential, such as cooperatives and condominiums.
. Class 3: Includes property with equipment owned by a gas, telephone or electric company.
. Class 4: Includes all other properties not included in class 1,2, and 3, such as offices, factories, warehouses, garage buildings, etc.


```{r}
boxplot(sale_price~tax_class, no_NA_data_trimmed)
```
We see that there are 11 factors in Tax_class where as ideally it should be only 1,2,3 & 4 as per the description. So we merge 1, 1A, 1B, 1C to 1 and 2, 2A, 2B, 2C to 2. 

```{r}
no_NA_data_trimmed$tax_class = ifelse ( no_NA_data_trimmed$tax_class %in% c("1","1A", "1B","1C"),1, ifelse(no_NA_data_trimmed$tax_class %in% c("2","2A", "2B","2C"),2,
ifelse(no_NA_data_trimmed$tax_class ==3,3, ifelse(no_NA_data_trimmed$tax_class == 4,4,0)))) 
```
```{r}
boxplot(sale_price~tax_class, no_NA_data_trimmed)
```

We see that tax_class 2 has some outliers and the sale_price is randomly distributed among the tax classes.

Block- 
A Tax Block is a sub-division of the borough on which real properties are located.
The Department of Finance uses a Borough-Block-Lot classification to label all real property in the City. "Whereas" addresses describe the street location of a property, the block and lot distinguishes one unit of real property from another, such as the different condominiums in a single building. Also, block and lots are not subject to name changes based on which side of the parcel the building puts its entrance on.


```{r}
plot(no_NA_data_trimmed$block, no_NA_data_trimmed $sale_price)
```
No correlation found.


Lot - 

Often the Tax Lot number can tell you the type of tax lot. The following table identifies some of these tax lot numbering conventions. Of course there are exceptions to each convention.

TAX LOT NUMBER    TYPE OF LOT
1-999           Traditional Tax Lots
1001-6999       Condominium Unit Lots
7501-7599       Condominium Billing Lots
8000-8899       Subterranean Tax lots
8900-8999       DTM Dummy Tax Lots
9000-9899       Air Rights Tax Lots

```{r}
plot(no_NA_data_trimmed$lot, no_NA_data_trimmed $sale_price)
```
No correlation found. in this raw plot. But we can create categories of TYPE OF PLOT as defind in the description.

```{r}
no_NA_data_trimmed$type_of_lot = ifelse( no_NA_data_trimmed$lot %in% 1:999, "Traditional Tax Lots", "Condominium Unit Lots")
```
Plotting Boxplots of Type of Plots.

```{r}
boxplot(sale_price~ type_of_lot, no_NA_data_trimmed)
```
From the above boxplot we can infer that the median of sale price of Traditional Tax lots is higher than the median sale price of condominium plots.


Building Class- 
This is a field that we are including so that users of the Rolling Sales Files can easily identify similar properties by broad usage (e.g. One Family Homes) without looking up individual Building Classes.

```{r}
boxplot(sale_price~building_class, no_NA_data_trimmed, col = "blue")
```

We can see that some building classes do not form any box. For better understanding, we count the number of rows in each column to check the discrepancy.

```{r}
bc = data.frame(Building_Class = levels(no_NA_data_trimmed$building_class), count = tabulate(no_NA_data_trimmed$building_class)) 

bc[order(bc$count),]
```
We find that many factors have been retained even after subsetting or indexing the data frame from the original data. These factors provide no information as they don't contain any data. We decide to drop them. We just need to apply factor again to our subsetted dataset.

```{r}
no_NA_data_trimmed$building_class = factor(no_NA_data_trimmed$building_class)
```
```{r}
bc = data.frame(building_class = levels(no_NA_data_trimmed$building_class), count = tabulate(no_NA_data_trimmed$building_class)) 

bc[order(bc$count),]
```
We further remove building classes that have less than  100 count as they will not give a good estimate of the sale price.

```{r}
no_NA_data_trimmed =  merge (no_NA_data_trimmed, bc , by = "building_class")

no_NA_data_trimmed = subset(no_NA_data_trimmed, no_NA_data_trimmed$count>100)
no_NA_data_trimmed$building_class = factor (no_NA_data_trimmed$building_class)
```
Again plotting the boxplot

```{r}
boxplot(sale_price~building_class,no_NA_data_trimmed, col= "gold")
```


Zip Code - 

The property's postal code

```{r}
plot(no_NA_data_trimmed$zip_code, no_NA_data_trimmed$sale_price )
```
WE find that some zipcodes have 0 value. This is inconsistent since zipcodes cannot have zero values. Let's remove the rows having zipcodes 0.

```{r}
no_NA_data_trimmed =  subset(no_NA_data_trimmed , zip_code != 0)
no_NA_data_trimmed$building_class = factor(no_NA_data_trimmed$building_class)

plot(no_NA_data_trimmed$zip_code, no_NA_data_trimmed$sale_price )
```
We find that there is no discernable co-relation.

Residential Units-
The number of residential units at the listed property.

```{r}
plot( no_NA_data_trimmed$residential_units, no_NA_data_trimmed$sale_price)
```
No correlation found.

Commercial Units:
The number of commercial units at the listed property.
```{r}
plot( no_NA_data_trimmed$commercial_units, no_NA_data_trimmed$sale_price)
```
No correlation found.

Total Units:
The total number of units at the listed property.

```{r}
plot(no_NA_data_trimmed$total_units, no_NA_data_trimmed$sale_price)
```
No Correlation found.

Land Sqft.
The land area of the property listed in square feet.


```{r}
plot(no_NA_data_trimmed$land_sqft, no_NA_data_trimmed$sale_price)
```

```{r}
attach(no_NA_data_trimmed)
library(ggplot2)
ggplot(no_NA_data_trimmed, aes(year_of_sale, mean(sale_price), color = type_of_lot)) + geom_line(aes(group = type_of_lot),col =  "black") + geom_point()

```
We need to use ggplot after grouping block wise or area wise.

Gross Sqft-
The total area of all the floors of a building as measured from the exterior surfaces of the outside walls of the building, including the land area and space within any building or structure on the property.

```{r}
plot(no_NA_data_trimmed$gross_sqft, no_NA_data_trimmed$sale_price)
```
The trend is similar to land sqft.

Year Built-
Year the structure on the property was built.

```{r}
plot(no_NA_data_trimmed$year_built, no_NA_data_trimmed$sale_price)
```
We find that many properties have Year field as 0. This information will be misleading for the model so we delete the observations having year as 0.

```{r}
no_NA_data_trimmed =  subset(no_NA_data_trimmed, year_built > 1800 )

no_NA_data_trimmed$building_class = factor(no_NA_data_trimmed$building_class)

plot(no_NA_data_trimmed$year_built, no_NA_data_trimmed$sale_price)
```
Tax Class at Sale-


```{r}
boxplot(sale_price~ tax_class_at_sale, no_NA_data_trimmed)

```
This plot is exactly same as Tax Class. So we remove this too.


Building_class_at_sale:
The Building Classification is used to describe a property's constructive use. The first position of the Building Class is a letter that is used to describe a general class of properties (for example "A" signifies one-family homes, "O" signifies office buildings. "R" signifies condominiums). The second position, a number, adds more specific information about the property's use or construction style (using our previous examples "A0" is a Cape Cod style one family home, "O4" is a tower type office building and "R5" is a commercial condominium unit). The term Building Class as used by the Department of Finance is interchangeable with the term Building Code as used by the Department of Buildings.

```{r}
no_NA_data_trimmed$building_class_at_sale = factor(no_NA_data_trimmed$building_class_at_sale)

boxplot(sale_price~ building_class_at_sale, no_NA_data_trimmed, col = "green")
```
This is same as building_class so we can remove this attribute.

```{r}
pairs(sale_price ~ year_of_sale+CT2010+ CB2010+CD, no_NA_data_trimmed)
```

```{r}
names(no_NA_data_trimmed)
```

```{r}
final_data = subset ( no_NA_data_trimmed, select = c(sale_price, building_class, category, tax_class, type_of_lot,zip_code, residential_units, gross_sqft,year_built, year_of_sale, CD, Council, LandUse,ResArea, GarageArea, StrgeArea, NumBldgs,NumFloors, BldgFront, BldgDepth, ProxCode,IrrLotCode, LotType, AssessTot, ExemptTot, YearAlter1, YearAlter2, BuiltFAR, XCoord, YCoord) )
```
```{r}
attach(no_NA_data_trimmed)
plot(GarageArea, sale_price)
     
```

Tree
```{r}
library(tree)
set.seed(1)
tree.brooklyn=tree(sale_price~. -building_class, data=final_data)
summary(tree.brooklyn)
plot(tree.brooklyn)
text(tree.brooklyn, pretty=0)
```

```{r}
#Boosting to check variable importance
install.packages("gbm")
final_data$category=as.factor(final_data$category)
final_data$type_of_lot=as.factor(final_data$type_of_lot)
library (gbm)
 set.seed (1)
 boost.final_data =gbm(sale_price???. -tax_class,data=final_data, distribution="gaussian",n.trees =5000 ,interaction.depth =4)
 summary(boost.final_data)
```

```{r}
#Removed the variable which are less important and obtained final data  with 17 attributes 
final_data2=subset ( final_data, select = c(sale_price, building_class, category,zip_code, gross_sqft,year_built, year_of_sale, CD, Council,ResArea, BldgFront, BldgDepth, AssessTot, ExemptTot, BuiltFAR, XCoord, YCoord) )
```

```{r}
#Using 70% of data as training set to fit the model and test the model on the remaining 30% data 
set.seed(100)
train=sample(1:nrow(final_data2), 0.7*(nrow(final_data2)))
test=final_data2[-train,"sale_price"]
dim(final_data2[train,])
dim(final_data2[-train,])
```



Total number of observations in test data= 150542
```{r}
n=45163
```

From here on we started fitting various models.

1. Boosting
```{r}
#Fitting the model
set.seed (1)
 boost.final_data2 =gbm(sale_price???.,data=final_data2[train,], distribution="gaussian",n.trees =5000 ,interaction.depth =4)
 summary(boost.final_data2)
 p=length(boost.final_data2$var.names)
 plot(boost.final_data2 ,i="XCoord",type="l")
```

```{r}
#Predicting sales price from he model
predict.boost=predict(boost.final_data2,newdata = final_data2[-train ,],
n.trees =5000)
mean(( predict.boost -test)^2)
```

```{r}
#Calculating R-Square Adjusted 
actual=final_data2$sale_price
r2.gradient_boosting=1-((sum((actual[-train] - predict.boost)^2)/(n-p-1))/(sum((actual[-train] - mean(actual[-train]))^2)/(n-1)))
r2.gradient_boosting
```

2.Random Forest
```{r}
#Fit the model
final_data2$category=as.factor(final_data2$category)
library(randomForest)
set.seed(1)
bag.brooklyn=randomForest(sale_price~.,data=final_data2, subset=train, mtry=4, ntree=10, importance=TRUE)
bag.brooklyn
p=16

#Predict sales price from the model
predict.bag = predict (bag.brooklyn,newdata =final_data2[-train ,])
plot(predict.bag ,final_data2$sale_price[-train])
abline (0,1)
mean((predict.bag -final_data2$sale_price[-train])^2)

#Calulating R-Square ajusted 
r2.random_forest =1-((sum((actual[-train] - predict.bag)^2)/(n-p-1))/(sum((actual[-train] - mean(actual[-train]))^2)/(n-1)))
r2.random_forest

```


3.KNN Regression
```{r}
install.packages("FNN")
R2.knn = numeric()

x = model.matrix(sale_price~., final_data2)[,-1]
y= final_data2$sale_price

knn001= FNN::knn.reg(train = x[train,], test = x[-train,], y = y[train], k = 1)

knn005= FNN::knn.reg(train = x[train,], test = x[-train,], y = y[train], k = 5)

knn010= FNN::knn.reg(train = x[train,], test = x[-train,], y = y[train], k = 10)

knn050= FNN::knn.reg(train = x[train,], test = x[-train,], y = y[train], k = 50)

knn060= FNN::knn.reg(train = x[train,], test = x[-train,], y = y[train], k = 60)

knn100= FNN::knn.reg(train = x[train,], test = x[-train,], y = y[train], k = 100)

knn1000= FNN::knn.reg(train = x[train,], test = x[-train,], y = y[train], k = 1000)


R2.knn[1] =1 - ((sum((actual[-train] - knn001$pred)^2)/(n-p-1))/(sum((actual[-train] - mean(actual[-train]))^2)/(n-1)))
R2.knn[2] =1 - ((sum((actual[-train] - knn005$pred)^2)/(n-p-1))/(sum((actual[-train] - mean(actual[-train]))^2)/(n-1)))
R2.knn[3] =1 - ((sum((actual[-train] - knn010$pred)^2)/(n-p-1))/(sum((actual[-train] - mean(actual[-train]))^2)/(n-1)))
R2.knn[4] =1 - ((sum((actual[-train] - knn050$pred)^2)/(n-p-1))/(sum((actual[-train] - mean(actual[-train]))^2)/(n-1)))
R2.knn[5] =1 - ((sum((actual[-train] - knn060$pred)^2)/(n-p-1))/(sum((actual[-train] - mean(actual[-train]))^2)/(n-1)))
R2.knn[6] =1 - ((sum((actual[-train] - knn100$pred)^2)/(n-p-1))/(sum((actual[-train] - mean(actual[-train]))^2)/(n-1)))
R2.knn[7] =1 - ((sum((actual[-train] - knn1000$pred)^2)/(n-p-1))/(sum((actual[-train] - mean(actual[-train]))^2)/(n-1)))


R2.knn

```


4. multiple linear regresion
```{r}
#Fit the model
install.packages("boot")
library(boot)
set.seed(10)
lm.fit2=lm(sale_price~.,data=final_data2[train,])
summary(lm.fit2)

#Predict the model
p=16
predict.lm2=predict(lm.fit2 , final_data2[-train,])
r2.lm=1-((sum((actual[-train] - predict.lm2)^2)/(n-p-1))/(sum((actual[-train] - mean(actual[-train]))^2)/(n-1)))
r2.lm

coef()


```

5. Ridge regression
```{r}
library(glmnet)
grid = 10^seq(10,-2, length = 100)

#fitting ridge model
ridge.sale_price = glmnet (x[train,],y[train],alpha= 0 , lambda = grid)
coef(ridge.sale_price)
p=12

#selecting best value of lambda for ridge
set.seed(1)
cv.out = cv.glmnet(x[train,],y[train], alpha = 0)
plot(cv.out)
bestlam.ridge = cv.out$lambda.min
bestlam.ridge

#Predict sales price using ridge regression
ridge.pred = predict (ridge.sale_price,s = bestlam.ridge, newx = x[-train,])
mean((ridge.pred - y[-train])^2)

#Calulating R-Square adjusted
R2.ridge =1-((sum((actual[-train] - ridge.pred)^2)/(n-p-1))/(sum((actual[-train] - mean(actual[-train]))^2)/(n-1))) 
R2.ridge
```

6. Lasso Regression
```{r}
#fitting the lasso
lasso.sale_price = glmnet(x[train,], y[train], alpha = 1, lambda = grid)
names(lasso.sale_price)

#selecting best value of lambda for lasso
set.seed(1)
cv.out = cv.glmnet(x[train,],y[train], alpha = 1)
plot(cv.out)
bestlam.lasso = cv.out$lambda.min
bestlam.lasso

#Predict sales price using lasso regression
lasso.pred = predict (lasso.sale_price, s = bestlam.lasso, newx = x[-train,])
mean((lasso.pred - y[-train])^2)

#Calculating R-Square adjusted
R2.lasso = 1-((sum((actual[-train] - lasso.pred)^2)/(n-p-1))/(sum((actual[-train] - mean(actual[-train]))^2)/(n-1))) 
R2.lasso


```

Results comparision
```{r}

Result_Comparison = data.frame( "Method" = c("Multiple Linear", "Ridge ", "Lasso", "KNN", "Boosting", "RandomForest"), "Adjusted_R-squared_Values"= c(r2.lm,R2.ridge,R2.lasso,R2.knn[4],r2.gradient_boosting,r2.random_forest))

barplot(Result_Comparison$Adjusted_R.squared_Values, col= "dodgerblue", names.arg = Result_Comparison$Method, main = "Result comparison", cex.names = 0.8, ylab = "Adjusted R-squared Values",xlab="Method applied for prediction")
```

Hence we see Bossting provides highest value of R-Square adjusted and so it is the best we have obtained for prediction of sales price




